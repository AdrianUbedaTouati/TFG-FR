[LOG] Escribiendo logs en: outputs/lstm_summary_line_cls\log\train_20250831-155806.log
Modelo seleccionado: LSTM (Clasificación Summary, *una línea -> clase*)
Modo: SIN ventanas deslizantes (seq_len=1).
[TOP-K] Clases seleccionadas (one-hot): ['Partly Cloudy', 'Mostly Cloudy'] | cobertura=63.23%
[TOP-K] Mantengo 60989/96453 filas (63.23%) pertenecientes al top-2 por frecuencia.
Dataset: data/weatherHistory_normalize.csv
Tiempo: Formatted Date | Clases: 2
Features (18): ['h_sin', 'h_cos', 'dow_sin', 'dow_cos', 'doy_sin', 'doy_cos', 'Precip Type_normalized', 'Humidity_normalized', 'Wind Speed (km/h)_normalized', 'wind_bearing_sin', 'wind_bearing_cos', 'Visibility (km)_normalized', 'Pressure (millibars)_normalized', 'trend_normalized', 'Temperature (C)_normalized', 'Apparent Temperature (C)_normalized', 'Summary_Mostly Cloudy', 'Summary_Partly Cloudy']
Nota: excluimos 'Summary_*' de FEATURES para evitar fuga de información.
Salida: outputs/lstm_summary_line_cls (guardado class_index.json)
[TOP-K] (one-hot) ['Partly Cloudy', 'Mostly Cloudy'] | cobertura=63.23%
[TOP-K] Mantengo 60989/96453 filas (63.23%) tras filtrar fuera del top-2.
Inicio entrenamiento — clases=2 | muestras: train=42692, val=9148, test=9149
Epoch 001 | train_loss=0.3850 | train_acc=0.8745 | train_f1=0.8723 | val_loss=0.0472 | val_acc=1.0000 | val_f1=1.0000
Epoch 002 | train_loss=0.0067 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 003 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 004 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 005 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 006 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 007 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 008 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 009 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 010 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 011 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 012 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 013 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 014 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 015 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 016 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 017 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 018 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 019 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 020 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 021 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 022 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 023 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 024 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 025 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 026 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 027 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 028 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 029 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
Epoch 030 | train_loss=0.0000 | train_acc=1.0000 | train_f1=1.0000 | val_loss=0.0000 | val_acc=1.0000 | val_f1=1.0000
[EarlyStopping] Sin mejora en 10 épocas. Mejor epoch=20 (val_loss=0.0000).
Test: loss=0.00000 | acc=1.0000 | f1_macro=1.0000
[CONFUSION] Guardada en: outputs/lstm_summary_line_cls\plots\confusion_matrix_raw_from_train.png
[CKPT] Guardado en: outputs/lstm_summary_line_cls\checkpoints\lstm_line_cls_20250831-155835.pt | [TorchScript] outputs/lstm_summary_line_cls\checkpoints\lstm_line_cls_20250831-155835.ts
[PLOTS] Artefactos guardados en: outputs/lstm_summary_line_cls\plots
Listo.
